{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ravi teja\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\users\\ravi teja\\anaconda3\\lib\\site-packages (from nltk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# pip install\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and download\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Chopping off a given document into small pieces is known as tokenization.\n",
    "\n",
    "These small pieces of text are called as tokens. \n",
    "\n",
    "Sentence tokenization chops a document or article into sentences.\n",
    "word tokenization chops a document or article down to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing tokenizers\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is python class.', 'I like python.', 'I am a student.', 'This is last line.']\n"
     ]
    }
   ],
   "source": [
    "# using sentence tokenizer\n",
    "\n",
    "example_text = \"This is python class. I like python.  I am a student. This is last line. \"\n",
    "sent = sent_tokenize(example_text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'python', 'class', '.', 'I', 'like', 'python', '.', 'I', 'am', 'a', 'student', '.', 'This', 'is', 'last', 'line', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer\n",
    "\n",
    "words = word_tokenize(example_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'python', 'class.', 'I', 'like', 'python.', 'I', 'am', 'a', 'student.', 'This', 'is', 'last', 'line.']\n"
     ]
    }
   ],
   "source": [
    "#using .split() function\n",
    "\n",
    "example_words = example_text.split() \n",
    "print (example_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing.\n",
    "\n",
    "There are three major ways of text preprocessing. \n",
    "1. Noise reduction\n",
    "2. Lexicon normalization\n",
    "3. Object standarization \n",
    "\n",
    "#### Noise reduction:\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "Ex: 'is' , 'or' , 'and' , 'the'. \n",
    "\n",
    "These words can also be called as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making a list of meaningless words\n",
    "\n",
    "noise_text = [\"is\",\"a\",\"for\", \"that\", \"this\" , \"it\" , \"of\" , \"to\"]\n",
    "\n",
    "# creating a function to remove noise\n",
    "\n",
    "def remove_noise(input_text):\n",
    "    \n",
    "    words = input_text.split()                                                #splitting the sentence into words\n",
    "    noise_free_words = [w for w in words if w not in noise_text]              # FOR loop to remove words in above list\n",
    "    noise_free_text = \" \".join(noise_free_words)                              # joining those words\n",
    "    return noise_free_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cricket bat. give bat Virat Kohli'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_noise(\" this is a cricket bat. give this bat to Virat Kohli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 2 :  stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing stopwords from nltk.corpus\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t', 'their', 'to', 'should', 'did', 'after', \"mustn't\", 'for', \"don't\", \"couldn't\", 'once', 'our', 'no', 'doesn', \"it's\", 'himself', 'how', 'such', 'i', 'were', 'not', 'again', 'd', \"mightn't\", 'she', 's', 'hasn', \"that'll\", 'against', 'more', \"should've\", 'aren', 'below', 'into', 'further', 'theirs', \"wouldn't\", 'it', 'but', 'myself', 'that', 'and', 'own', 'just', 'ain', 'has', 'being', \"didn't\", 'between', 'wouldn', 'my', 'which', 'will', \"aren't\", 'above', 'mustn', 'are', 'whom', 'now', 'do', 'a', 'couldn', \"shouldn't\", \"haven't\", 'off', 'these', 'with', 'shouldn', 'few', 'this', 'than', 'who', 'when', 'same', 'too', \"wasn't\", 'because', 're', \"doesn't\", 'other', 'have', 'ourselves', 'there', 'herself', 'themselves', 'at', 'about', 'we', 'out', 'under', 'll', 'nor', 'the', \"hasn't\", 'won', 'having', \"you'd\", 'only', 'hadn', 'its', 'yourselves', 'can', 'm', 'from', 'on', 'mightn', 'until', \"you've\", 'ma', 'each', 'me', 'what', 'those', 'while', 'then', 'haven', 'why', 'didn', \"weren't\", 'in', 'or', 'here', 'so', 'an', 'her', \"hadn't\", 'as', 'wasn', 'ours', 'very', 'some', 'his', 'was', 'shan', 'is', 'don', 'of', 'over', 'your', \"needn't\", 'isn', 'you', 'doing', 'by', 'y', 'needn', 'most', 'him', 'yours', 'yourself', 'does', 'through', 'before', 'if', \"shan't\", \"isn't\", 'both', \"won't\", 'itself', 'them', 'up', 'o', 'they', 've', 'been', \"you'll\", 'am', 'any', \"she's\", 'be', 'he', \"you're\", 'down', 'during', 'had', 'weren', 'all', 'hers', 'where'}\n"
     ]
    }
   ],
   "source": [
    "#making english stopwords into a list\n",
    "\n",
    "stop_words = set(stopwords.words(\"English\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to remove stopwords\n",
    "\n",
    "def noiseless_text(input_text):\n",
    "    \n",
    "    words = word_tokenize(input_text)                     #splitting sentence into words\n",
    "    noiseless_text =[]                                    # making a empty list\n",
    "    \n",
    "    for w in words:                                       # FOR loop to remove words from above list\n",
    "         if w not in stop_words:\n",
    "                noiseless_text.append(w)                  # appending remaining words into list\n",
    "    return print(noiseless_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'fought', 'second', 'world', 'war', '.', 'India', 'sent', 'soliders', 'supplies', 'war']\n"
     ]
    }
   ],
   "source": [
    "noiseless_text(\"India fought during second world war. India sent soliders and supplies into war\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon normalization\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "Example :  write, wrote, writing, writer, written \n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "#### Stemming:  \n",
    "\n",
    "Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automation', 'automatic', 'automated', 'automotive']\n"
     ]
    }
   ],
   "source": [
    "# lower casing the words\n",
    "\n",
    "example = \"Automation automatic automated automotive\"\n",
    "\n",
    "example_lower = example.lower().split()\n",
    "print(example_lower)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stemming\n",
    "#import stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autom\n",
      "automat\n",
      "autom\n",
      "automot\n"
     ]
    }
   ],
   "source": [
    "#FOR loop for using Porter Stemmer\n",
    "\n",
    "for word in example_lower:\n",
    "    stemmed_word = ps.stem(word)\n",
    "    print(stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Lemmatization:\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import lemmatizer \n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example2 = [\"maker\", \"called\" , \"ears\", \"loving\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make\n",
      "called\n",
      "ear\n",
      "loving\n"
     ]
    }
   ],
   "source": [
    "#FOR loop for using lemmatizer\n",
    "\n",
    "for word in example2:\n",
    "    lemmatized_word =  lem.lemmatize(word)\n",
    "    print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast\n",
      "fast\n",
      "fast\n"
     ]
    }
   ],
   "source": [
    "#lemmatizer with POS as 'adjective' example\n",
    "\n",
    "ex_3 =  [\"fast\", \"faster\",\"fastest\"]\n",
    "for word in ex_3:\n",
    "    lemmatized_word =  lem.lemmatize(word, pos = 'a')\n",
    "    print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
